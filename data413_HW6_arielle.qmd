---
title: "Homework Assignment: Sentiment Analysis of Emma"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

**Title**: Exploring Joyful Language in Jane Austen’s *Emma* using Tidytext


### **Instructions**


Complete the following exercises using the appropriate packages in R. Ensure that your solutions are optimized and use functional programming principles where applicable.

1.  Load the necessary libraries.
2.  Answer each question in separate R code chunks.
3.  Provide detailed explanations for your approach.
4.  Submit the rendered HTML file.

install.packages("wordcloud")

```{r}
#load necessary libraries
library(janeaustenr)   
library(dplyr)         
library(stringr)       
library(tidytext)  
library(textdata)
library(ggplot2)
library(wordcloud)
```

####  **Objective**:

Use the **`tidytext`** package and three different **sentiment lexicons** (`nrc`, `afinn`, `bing`) to explore **positive/joyful words** in *Emma* by Jane Austen. You will tokenize the text, apply sentiment filters, visualize frequent sentiment words using `ggplot2`, and create a word cloud.


### **Tasks**:

1. **Data Preparation** 

   * Load the `austen_books()` dataset from the **`janeaustenr`** package.
   * Group by book and detect chapter boundaries using regex.
   * Create `linenumber` and `chapter` columns.
   
```{r}
emma_data <- austen_books() |>
  #keep only Emma
  filter(book == "Emma")|>
  #group by book
  group_by(book) |> 
  #adds line numbers
  mutate(linenumber = row_number(),
  #detects chapters by regex and has chapter counter
  chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",ignore_case = TRUE)))) 

head(emma_data, 15)
```

2. **Tokenization** 

   * Use `unnest_tokens()` to tokenize text into individual words.
   
```{r}
#unnest will tokenize the text into words, new column
emma_words <- emma_data |> unnest_tokens(word, text) 

head(emma_words, 15) 
```

3. **Sentiment Analysis** 

   * Filter joy/positive words from **each** of the three sentiment lexicons:

     * `nrc` (joy)
     * `afinn` (positive scores ≥ 1)
     * `bing` (positive)
     
```{r}
#load the required lexicons
nrc <- get_sentiments("nrc")  
afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")   
```

```{r}
#NRC joy words
emma_nrc_joy <- emma_words |>
  inner_join(nrc |> 
               filter(sentiment == "joy"), by = "word")

head(emma_nrc_joy)
```

```{r}
#AFIN positive score above 1
emma_afinn_pos <- emma_words|>
  inner_join(afinn|> 
               filter(value >= 1), by = "word")

head(emma_afinn_pos)
```

```{r}
#BING positive 

emma_bing_pos <- emma_words|>
  inner_join(bing |>
               filter(sentiment == "positive"), by= "word")

head(emma_bing_pos)
```

     
   * Join each with *Emma*'s text and:

     * Count word frequency.
     * Filter for frequently occurring words (`n > 50`).
     * Visualize using a **bar chart** (`ggplot2`) and a **word cloud** (`wordcloud`).

```{r}
#count of word frequency for each lexicon

#NRC
emma_nrc_counts <- emma_nrc_joy |>
  count(word, sort = TRUE)

#AFINN
emma_afinn_counts <- emma_afinn_pos |>
  count(word, sort =TRUE)

#BING
emma_bing_counts <- emma_bing_pos |>
  count(word, sort =TRUE)

head(emma_nrc_counts)
head(emma_afinn_counts)
head(emma_bing_counts)
```

```{r}
#filter frequently occuring words n>50

nrc_freq <-emma_nrc_counts |>
  filter(n > 50)

afinn_freq <- emma_afinn_counts |>
  filter(n>50)

bing_freq <-  emma_bing_counts|>
  filter(n > 50)

head(nrc_freq)
head(afinn_freq)
head(bing_freq)
```

4. **Push to GitHub** 

   * Push your complete R script (`.R` file) to a **GitHub repository**.
   * Your script should include:

     * Data wrangling
     * Sentiment filtering and joins
     * Visualization code
   * You must include **at least 5 meaningful commits** to document your workflow.



